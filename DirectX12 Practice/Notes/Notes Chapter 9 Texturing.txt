Texture mapping is a technique that allows us to map image data onto a triangle, thereby enabling us to increase the details
and realism of our scene significantly. For instance, we can build a cube and turn it into a crate by mapping a crate texture
on each side.

Textures are different than buffer resources, which just store arrays of data; textures can have mipmap levels, and the GPU can
do special operations on them, such as apply filters and multisampling. Because of these special operations that are supported
for texture resources, they are limited to certain kind of data formats, whereas buffer resources can store arbitrary data.

The data formats supported for textures are described by the DXGI_FORMAT enumerated type. Some example formats are:

1. DXGI_FORMAT_R32G32B32_FLOAT: Each element has three 32-bit floating-point components.
2. DXGI_FORMAT_R16G16B16A16_UNORM: Each element has four 16-bit components mapped to the [0, 1] range.
3. DXGI_FORMAT_R32G32_UINT: Each element has two 32-bit unsigned integer components.
4. DXGI_FORMAT_R8G8B8A8_UNORM: Each element has four 8-bit unsigned components mapped to the [0, 1] range.
5. DXGI_FORMAT_R8G8B8A8_SNORM: Each element has four 8-bit signed components mapped to the [-1, 1] range.
6. DXGI_FORMAT_R8G8B8A8_SINT: Each element has four 8-bit signed integer components mapped to the [-128, 127] range.
7. DXGI_FORMAT_R8G8B8A8_UINT: Each element has four 8-bit unsigned integer components mapped to the [0, 255] range.

A texture can be bound to different stages of the rendering pipeline; a common example is to use a texture as a render target
(i.e., Direct3D draws into the texture) and as a shader resource (i.e., the texture will be sampled in a shader).

Rendering to a texture and then using it as a shader resource, a method called render-to-texture, allows for some interesting
special effects.

For a texture to be used as both a render target and a shader resource, we would need to create two descriptors to that texture
resource: 1) one that lives in a render target heap (i.e., D3D12_DESCRIPTOR_HEAP_TYPE_RTV) and 2) one that lives in a shader
resource heap (i.e., D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV).

Then the resource can be bound as a render target or bound as a shader input to a root parameter in the root signature (but
never at the same time).

Resource descriptors essentially do two things: they tell Direct3D how the resource will be used (i.e., what stage of the
pipeline you will bind it to), and if the resource format was specified as typeless at creation time, then we must now state
the type when creating a view. Thus, with typeless formats, it is possible for the elements of a texture to be viewed as
floating-point values in one pipeline stage and as integers in another; this essentially amounts to a reinterpret cast of the
data.

Direct3D uses a texture coordinate system that consists of a u-axis that runs horizontally to the image and a v-axis that runs
vertically to the image.

The coordinates, (u, v) such that 0 < u, v < 1, identify an element on the texture called a texel.

Also, notice the normalized coordinate interval, [0, 1], which is used because it gives Direct3D a dimension independent range
to work with; for example, (0.5, 0.5) always specifies the middle texel no matter if the actual texture dimensions are 
256 x 256, 512 x 1024 or 2048 x 2048 in pixels.

We can map only a subset of a texture onto geometry. In fact, we can place several unrelated images on one big texture map
(this is called a texture atlas), and use it for several different objects. The texture coordinates are what will determine
what part of the texture gets mapped on the triangles.

The DDS (DirectDraw Surface format) format is ideal for 3D graphics because it supports special formats and texture types that
are specifically used for 3D graphics. It is essentially an image format built for GPUs.

For example, DDS textures support the following features (not yet discussed) used in 3D graphics development:

1. mipmaps
2. compressed formats that the GPU can natively decompress
3. texture arrays
4. cube maps
5. volume textures

The DDS format can support different pixel formats. The pixel format is described by a member of the DXGI_FORMAT enumerated
type; however, not all formats apply to DDS textures. Typically, for uncompressed image data you will use the formats:

1. DXGI_FORMAT_B8G8R8A8_UNORM or DXGI_FORMAT_B8G8R8X8_UNORM: For lowdynamic-range images.
2. DXGI_FORMAT_R16G16B16A16_FLOAT: For high-dynamic-range images.

The GPU memory requirements for textures add up quickly as your virtual worlds grow with hundreds of textures (remember we need
to keep all these textures in GPU memory to apply them quickly). To help alleviate these memory requirements, Direct3D supports
compressed texture formats: BC1, BC2, BC3, BC4, BC5, BC6, and BC7:

1. BC1 (DXGI_FORMAT_BC1_UNORM): Use this format if you need to compress a format that supports three color channels, and only a
1-bit (on/off) alpha component.

2. BC2 (DXGI_FORMAT_BC2_UNORM): Use this format if you need to compress a format that supports three color channels, and only
a 4-bit alpha component.

3. BC3 (DXGI_FORMAT_BC3_UNORM): Use this format if you need to compress a format that supports three color channels, and a
8-bit alpha component.

4. BC4 (DXGI_FORMAT_BC4_UNORM): Use this format if you need to compress a format that contains one color channel (e.g., a
grayscale image).

5. BC5 (DXGI_FORMAT_BC5_UNORM): Use this format if you need to compress a format that supports two color channels.

6. BC6 (DXGI_FORMAT_BC6_UF16): Use this format for compressed HDR (high dynamic range) image data.

7. BC7 (DXGI_FORMAT_BC7_UNORM): Use this format for high quality RGBA compression. In particular, this format significantly
reduces the errors caused by compressing normal maps.

A compressed texture can only be used as an input to the shader stage of the rendering pipeline, not as a render target.

Again, the advantage of these formats is that they can be stored compressed in GPU memory, and then decompressed on the fly by
the GPU when needed. An additional advantage of storing your textures compressed in DDS files is that they also take up less
hard disk space.

Once a texture resource is created, we need to create an SRV descriptor to it which we can set to a root signature parameter
slot for use by the shader programs. In order to do that, we first need to create a descriptor heap with 
ID3D12Device::CreateDescriptorHeap to store the SRV descriptors.

Once we have an SRV heap, we need to create the actual descriptors. An SRV descriptor is described by filling out a
D3D12_SHADER_RESOURCE_VIEW_DESC object, which describes how the resource is used and other information—its format, dimension,
mipmaps count, etc.

The idea of texturing mapping is to get the material data from texture maps instead of the material constant buffer. This
allows for per pixel variation which increases the details and realism of our scene.

A texture resource can actually be used by any shader (vertex, geometry, or pixel shader).

Texture atlases can improve performance because it can lead to drawing more geometry with one draw call. For example, suppose
we used the texture atlas as that contains the crate, grass, and brick textures. Then, by adjusting the texture coordinates for
each object to its corresponding subtexture, we could put all the geometry in one render item.

The elements of a texture map should be thought of as discrete color samples from a continuous image; they should not be
thought of as rectangles with areas.

Given the colors at the texels we can approximate the colors between texels using interpolation. There are two methods of
interpolation graphics hardware supports: constant interpolation and linear interpolation. In practice, linear interpolation
is almost always used.

2D linear interpolation is called bilinear interpolation.

Figure 9.7 - We zoom in on a cube with a crate texture so that magnification occurs. On the left we use constant interpolation,
which results in a blocky appearance; this makes sense because the interpolating function has discontinuities (Figure 9.5a),
which makes the changes abrupt rather than smooth. On the right we use linear filtering, which results in a smoother image due
to the continuity of the interpolating function.

In the context of texturing, using constant interpolation to find texture values for texture coordinates between texels is also
called point filtering. And using linear interpolation to find texture values for texture coordinates between texels is also
called called linear filtering. Point and linear filtering is the terminology Direct3D uses.

Minification is the opposite of magnification. In minification, too many texels are being mapped to too few pixels. For
instance, consider the following situation where we have a wall with a 256 x 256 texture mapped over it. The eye, looking at
the wall, keeps moving back so that the wall gets smaller and smaller until it only covers 64 x 64 pixels on screen. So now we
have 256 x 256 texels getting mapped to 64 x 64 screen pixels.

Another type of filter that can be used is called anisotropic filtering. This filter helps alleviate the distortion that occurs
when the angle between a polygon’s normal vector and camera’s look vector is wide (e.g., when a polygon is orthogonal to the
view window). This filter is the most expensive, but can be worth the cost for correcting the distortion artifacts.

A texture, combined with constant or linear interpolation, defines a vector-valued function T(u, v) = (r, g, b, a). That is,
given the texture coordinates (u, v) = [0, 1]^2 the texture function T returns a color (r, g, b, a).

Direct3D allows us to extend the domain of this function in four different ways (called address modes): wrap, border color,
clamp, and mirror.

1. wrap extends the texture function by repeating the image at every integer junction.
2. border color extends the texture function by mapping each (u, v) not in [0, 1]^2 to some color specified by the programmer.
3. clamp extends the texture function by mapping each (u, v) not in [0, 1]^2 to the color T(u0, v0), where (u0, v0) is the
nearest point to (u, v) contained in [0, 1]^2.
4. mirror extends the texture function by mirroring the image at every integer junction.

The wrap address mode is probably the most often employed; it allows us to tile a texture repeatedly over some surface. This
effectively enables us to increase the texture resolution without supplying additional data.

What filter and address mode to use when sampling a texture resource is defined by a sampler object. An application will
usually need several sampler objects to sample textures in different ways.

In order to bind samplers to shaders for use, we need to bind descriptors to sampler objects.

If we are going to be setting sampler descriptors, we need a sampler heap. A sampler heap is created by filling out a
D3D12_DESCRIPTOR_HEAP_DESC instance and specifying the heap type D3D12_DESCRIPTOR_HEAP_TYPE_SAMPLER.

Once we have a sampler heap, we can create sampler descriptors. It is here that we specify the address mode and filter type, as
well as other parameters by filling out a D3D12_SAMPLER_DESC object.

Below are some examples of commonly used D3D12_FILTER types:

1. D3D12_FILTER_MIN_MAG_MIP_POINT: Point filtering over a texture map, and point filtering across mipmap levels (i.e., the
nearest mipmap level is used).

2. D3D12_FILTER_MIN_MAG_LINEAR_MIP_POINT: Bilinear filtering over a texture map, and point filtering across mipmap levels
(i.e., the nearest mipmap level is used).

3. D3D12_FILTER_MIN_MAG_MIP_LINEAR: Bilinear filtering over a texture map, and linear filtering between the two nearest lower
and upper mipmap levels. This is often called trilinear filtering.

4. D3D12_FILTER_ANISOTROPIC: Anisotropic filtering for minification, magnification, and mipmapping.

Static samplers are described by the D3D12_STATIC_SAMPLER_DESC structure.

In addition, you can only define 2032 number of static samplers, which is more than enough for most applications. If you do
need more, however, you can just use non-static samplers and go through a sampler heap.

A texture object is defined in HLSL and assigned to a texture register with the following syntax:
Texture2D gDiffuseMap : register(t0);

Note that texture registers use specified by tn where n is an integer identifying the texture register slot. The root signature
definition specifies the mapping from slot parameter to shader register; this is how the application code can bind an SRV to a
particular Texture2D object in a shader.

SamplerStates (samplr objects in HLSL) correspond to the static sampler array we set in the previous section. Note that texture
registers use specified by sn where n is an integer identifying the sampler register slot.