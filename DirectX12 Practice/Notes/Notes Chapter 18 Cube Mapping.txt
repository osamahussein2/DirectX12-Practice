Cube maps are basically arrays of six textures interpreted in a special way. With cube mapping, we can easily texture a sky
or model reflections.

The idea of cube mapping is to store six textures and to visualize them as the faces of a cube - hence the name cube map -
centered and axis aligned about some coordinate system. Since the cube texture is axis aligned, each face corresponds
with a direction along the three major axes; therefore, it is natural to a reference a particular face on a cube map based
on the axis direction (+-X, +-Y, +-Z) that intersects the face.

In 3D the square becomes a cube. The square denotes the cube map centered and axis-aligned with some coordinate system. We
shoot a vector v from the origin. The texel v intersects is the sampled texel. In this illustration, v intersects the cube
face corresponding to the +Y axis.

In Direct3D, a cube map is represented by a texture array with six elements such that

1. index 0 refers to the +X face
2. index 1 refers to the –X face
3. index 2 refers to the +Y face
4. index 3 refers to the –Y face
5. index 4 refers to the +Z face
6. index 5 refers to the –Z face

In contrast to 2D texturing, we can no longer identify a texel with 2D texture coordinates. To identify a texel in a cube
map, we use 3D texture coordinates, which define a 3D lookup vector v originating at the origin. The texel of the cube map
that v intersects is the texel corresponding to the 3D coordinates of v.

The magnitude of the lookup vector is unimportant, only the direction matters. Two vectors with the same direction but
different magnitudes will sample the same point in the cube map.

In the HLSL, a cube texture is represented by the TextureCube type. The following code fragment illustrates how we sample a
cube map:

TextureCube gCubeMap;
SamplerState gsamLinearWrap : register(s2);

The lookup vector should be in the same space the cube map is relative to. For example, if the cube map is relative to the
world space (i.e., the cube faces are axis aligned with the world space axes), then the lookup vector should have world
space coordinates.

The primary application of cube maps is environment mapping. The idea is to position a camera at the center of some object
O in the scene with a 90 degree field of view angle (both vertically and horizontally). Then have the camera look down the
positive x-axis, negative x-axis, positive y-axis, negative y-axis, positive z-axis, and negative z-axis, and to take a
picture of the scene (excluding the object O) from each of these six viewpoints.

Because the field of view angle is 90 degrees, these six images will have captured the entire surrounding environment from
the perspective of the object O. We then store these six images of the surrounding environment in a cube map, which leads to
the name environment map. In other words, an environment map is a cube map where the cube faces store the surrounding images
of an environment.

If the axis directions the camera looked down to build the environment map images were the world space axes, then the
environment map is said to be generated relative to the world space. You could, of course, capture the environment from a
different orientation (say the local space of an object). However, the lookup vector coordinates must be in the space the
cube map is relative to.

Because cube maps just store texture data, their contents can be pre-generated by an artist (just like the 2D textures we’ve
been using). Consequently, we do not need to use real-time rendering to compute the images of a cube map. That is, we can
create a scene in a 3D world editor, and then pre-render the six cube map face images in the editor.

Once you have created the six cube map images using some program, we need to create a cube map texture, which stores all
six. The DDS texture image format we have been using readily supports cube maps, and we can use the texassemble tool to
build a cube map from six images.

Our DDS texture loading code (DDSTextureLoader.h/.cpp) already supports loading cube maps, and we can load the texture like
any other. The loading code will detect that the DDS file contains a cube map, and will create a texture array and load the
face data into each element.

When we create an SRV to a cube map texture resource, we specify the dimension D3D12_SRV_DIMENSION_TEXTURECUBE and use the
TextureCube property of the SRV description.

We can use an environment map to texture a sky. We create a large sphere that surrounds the entire scene. To create the
illusion of distant mountains far in the horizon and a sky, we texture the sphere using an environment map by the method
of: in 3D the square becomes a cube and the circle becomes a sphere. We assume that the sky and environment map are centered
about the same origin. Then to texture a point on the surface of the sphere, we use the vector from the origin to the
surface point as the lookup vector into the cube map. This projects the cube map onto the sphere. In this way, the
environment map is projected onto the sphere’s surface.

We assume that the sky sphere is infinitely far away (i.e., it is centered about the world space but has infinite radius),
and so no matter how the camera moves in the world, we never appear to get closer or farther from the surface of the sky
sphere. To implement this infinitely faraway sky, we simply center the sky sphere about the camera in world space so that it
is always centered about the camera. Consequently, as the camera moves, we are getting no closer to the surface of the
sphere. If we did not do this, and we let the camera move closer to the sky surface, the whole illusion would break down, as
the trick we use to simulate the sky would be obvious.

In the past, applications would draw the sky first and use it as a replacement to clearing the render target and depth/
stencil buffer. However, the "ATI Radeon HD 2000 Programming Guide" (http://developer.amd.com/media/gpu_assets/ATI_Radeon_HD_2000_programming_guide.pdf)
now advises against this for the following reasons. First, the depth/stencil buffer needs to be explicitly cleared for
internal hardware depth optimizations to perform well. The situation is similar with render targets. Second, typically most
of the sky is occluded by other geometry such as buildings and terrain. Therefore, if we draw the sky first, then we are
wasting resources by drawing pixels that will only get overridden later by geometry closer to the camera. Therefore, it is
now recommended to always clear, and to draw the sky last.

Drawing the sky requires different shader programs, and hence a new PSO.

In addition, rendering the sky requires some different render states.

Specular highlights come from light sources where the emitted light strikes a surface and can reflect into the eye based on
the Fresnel effect and surface roughness. However, due to light scattering and bouncing, light really strikes a surface from
all directions above the surface, not just along the rays from direct light sources.

By specular reflections, we mean that we are just going to look at the light that is reflected off a surface due to the
Fresnel effect.

When we render a scene about a point O to build an environment map, we are recording light values coming in from all
directions about the point O. In other words, the environment map stores the light values coming in from every direction
about the point O, and we can think of every texel on the environment map as a source of light. We use this data to
approximate specular reflections of light coming from the surrounding environment.

Here E is the eye point, and n is the surface normal at the point p. The texel that stores the light that reflects off p and
enters the eye is obtained by sampling the cube map with the vector r.

Light from the environment comes in with incident direction I and reflects off the surface (due to the Fresnel effect) and
enters the eye in the direction v = E - p. The light from the environment is obtained by sampling the environment cube map
with the lookup vector r = reflect(-v, n). This makes the surface have mirror like properties: the eye looks at p and sees
the environment reflected off p.

Fresnel effect determines how much light is reflected from the environment into the eye based on the material properties of
the surface and the angle between the light vector (reflection vector) and normal. In addition, we scale the amount of
reflection based on the shininess of the material - a rough material should have a low amount of reflection, but still some
reflection.

Reflections via environment mapping do not work well for flat surfaces. This is because the reflection vector does not tell
the whole story, as it does not incorporate position; we really need a reflection ray and to intersect the ray with the
environment map. A ray has position and direction, whereas a vector just has direction.

However, suppose that we want animated actors moving in our scene. With a pre-generated cube map, you cannot capture these
animated objects, which means we cannot reflect animated objects. To overcome this limitation, we can build the cube map at
runtime. That is, every frame you position the camera in the scene that is to be the origin of the cube map, and then render
the scene six times into each cube map face along each coordinate axis direction. Since the cube map is rebuilt every frame,
it will capture animated objects in the environment, and the reflection will be animated as well.

Rendering a cube map dynamically is expensive. It requires rendering the scene to six render targets! Therefore, try to
minimize the number of dynamic cube maps needed in a scene. For example, perhaps only use dynamic reflections for key
objects in your scene that you want to show off or accentuate. Then use static cube maps for the less important objects
where dynamic reflections would probably go unnoticed or not be missed. Normally, low resolution cube maps are used for
dynamic cube maps, such as 256 × 256, to save on pixel processing (i.e., fill rate).

The camera is placed at position O in the scene, centered about the object we want to generate the dynamic cube map relative
to. We render the scene six times along each coordinate axis direction with a field of view angle of 90 degrees so that the
image of the entire surrounding environment is captured.

In addition, we will need one extra SRV so that we can bind the cube map as a shader input after it has been generated.

In the previous section, we allocated heap space for our descriptors and cached references to the descriptors, but we did
not actually create any descriptors to resources. We now need to create an SRV to the cube map resource so that we can
sample it in a pixel shader after it is built, and we also need to create a render target view to each element in the cube
map texture array, so that we can render onto each cube map face one-by-one.

Generally, the cube map faces will have a different resolution than the main back buffer. Therefore, for rendering to the
cube map faces, we need a depth buffer with dimensions that matches the resolution of a cube map face. However, because we
render to the cube faces one at a time, we only need one depth buffer for the cube map rendering.

Draw calls are not free, and we should work to minimize them. There is a Direct3D 10 sample called "CubeMapGS," which uses
the geometry shader to render a cube map by drawing the scene only once.

Assigning a triangle to a render target array slice is done by setting the system value SV_ RenderTargetArrayIndex. This
system value is an integer index value that can only be set as an output from the geometry shader to specify the index of
the render target array slice the primitive should be rendered onto. This system value can only be used if the render
target view is actually a view to an array resource. 

A dielectric is a transparent material that refracts light. When a ray strikes a dielectric, some light reflects and some
light refracts based on Snell’s Law of Refraction. The indices of refraction n1 and n2 determine how much the light bends:

1. If n1 = n2, then theta1 = theta2 (no bending).
2. If n2 > n1, then theta2 < theta1 (ray bends toward normal).
3. If n1 > n2, then theta2 > theta1 (ray bends away from normal).